# -*- coding: utf-8 -*-
"""sentiment analysis distilBERTl.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M4-HSV--ahi6AE5Skhmu06Loqmq-N9lb
"""

import pandas as pd
import numpy as np
import torch
from sklearn.model_selection import train_test_split as split
from sklearn.metrics import r2_score, root_mean_squared_error, mean_absolute_error
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import TrainingArguments
from transformers import Trainer
import joblib

test = pd.read_csv(r"/content/drugsComTest_raw.csv")
train = pd.read_csv(r"/content/drugsComTrain_raw.csv")

X = train["review"]
y = train["rating"]
X_test = test["review"]
y_test = test["rating"]

X_train, X_val, y_train, y_val = split(X, y, test_size=0.2, random_state=42)

model_name = "distilbert-base-uncased"

tokenizer = AutoTokenizer.from_pretrained(model_name)

max_length = 128

class ReviewsDataset(torch.utils.data.Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = list(texts)
        self.labels = list(labels)
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = float(self.labels[idx])

        encoding = self.tokenizer(
            text,
            padding="max_length",
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt"
        )

        item = {k: v.squeeze(0) for k, v in encoding.items()}
        item["labels"] = torch.tensor(label, dtype=torch.float)
        return item

train_dataset = ReviewsDataset(X_train, y_train, tokenizer, max_length)
val_dataset   = ReviewsDataset(X_val,   y_val,   tokenizer, max_length)
test_dataset  = ReviewsDataset(X_test,  y_test,  tokenizer, max_length)

model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=1,
    problem_type="regression"
    )


def compute_metrics(eval_pred):
    preds, labels = eval_pred
    # preds shape: (batch_size, 1), squeeze to (batch_size,)
    preds = preds.reshape(-1)
    labels = labels.reshape(-1)

    rmse = root_mean_squared_error(labels, preds)
    mae = mean_absolute_error(labels, preds)
    r2 = r2_score(labels, preds)

    return {
        "rmse": rmse,
        "mae": mae,
        "r2": r2
    }


training_args = TrainingArguments(
    output_dir="./bert-drug-rating",
    eval_strategy="epoch",          # âœ… <- this matches your signature
    logging_strategy="epoch",       # if you want logs once per epoch
    save_strategy="epoch",          # save checkpoint per epoch
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    num_train_epochs=3,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="rmse",
    greater_is_better=False,
    report_to="none",               # avoids TB/W&B warnings
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)


trainer.train()

metrics = trainer.evaluate(test_dataset)
print(metrics)

print(trainer.state.best_model_checkpoint)

!zip -r bert-drug-rating-final.zip bert-drug-rating/checkpoint-24195

from google.colab import files
files.download("bert-drug-rating-final.zip")